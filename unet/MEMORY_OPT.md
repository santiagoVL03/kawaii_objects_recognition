# Ejemplo de uso del dataset eficiente en memoria

## 游 Soluci칩n al problema de memoria RAM

El problema que ten칤as era que estabas cargando **todas las im치genes** en memoria de una vez con tu funci칩n `PreprocessData()`. Esto es muy ineficiente y causa problemas de memoria.

## Nueva soluci칩n con tf.data.Dataset

Ahora uso `tf.data.Dataset` que:
- **Carga las im치genes solo cuando las necesita** (lazy loading)
- **Procesa las im치genes en paralelo** para mejor rendimiento
- **Usa prefetching** para preparar el siguiente batch mientras entrena el actual
- **Optimiza autom치ticamente** el uso de memoria

## 游늶 C칩mo usar la nueva implementaci칩n:

### Opci칩n 1: Usar el archivo modificado
```python
python train.py  # Tu archivo modificado
```

### Opci칩n 2: Usar el archivo optimizado (recomendado)
```python
python train_efficient.py  # Versi칩n completamente optimizada
```

### Opci칩n 3: Usar manualmente en tu c칩digo
```python
from unet.utils.dataset_efficient import create_train_validation_datasets

# Crear datasets eficientes
train_dataset, val_dataset = create_train_validation_datasets(
    images_dir="path/to/images",
    masks_dir="path/to/masks", 
    target_shape_img=[128, 128, 3],
    target_shape_mask=[128, 128, 1],
    batch_size=16,
    validation_split=0.2
)

# Entrenar directamente
model.fit(train_dataset, validation_data=val_dataset, epochs=10)
```

## Ventajas de la nueva implementaci칩n:

1. **Memoria constante**: Solo mantiene en memoria un batch a la vez
2. **Procesamiento paralelo**: Las im치genes se procesan usando m칰ltiples cores
3. **Prefetching**: Mientras entrena un batch, prepara el siguiente
4. **Reproducible**: Usa seeds para resultados consistentes
5. **Flexible**: F치cil de modificar batch size y par치metros

## 丘뙖잺 Optimizaciones adicionales incluidas:

- **Callbacks inteligentes**: ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
- **Gesti칩n autom치tica de GPU**: Configura memoria din치mica si hay GPU disponible
- **Visualizaciones**: Genera gr치ficos autom치ticos del entrenamiento
- **Logging mejorado**: Informaci칩n clara del progreso

## 游댢 Par치metros configurables:

```python
batch_size = 16        # Ajusta seg칰n tu RAM/GPU
validation_split = 0.2 # 20% para validaci칩n
target_shape = [128, 128, 3]  # Tama침o de imagen
```

**Resultado**: Tu entrenamiento usar치 mucha menos RAM y ser치 m치s eficiente! 游꿀
