# Ejemplo de uso del dataset eficiente en memoria

## ğŸš€ SoluciÃ³n al problema de memoria RAM

El problema que tenÃ­as era que estabas cargando **todas las imÃ¡genes** en memoria de una vez con tu funciÃ³n `PreprocessData()`. Esto es muy ineficiente y causa problemas de memoria.

## âœ… Nueva soluciÃ³n con tf.data.Dataset

Ahora uso `tf.data.Dataset` que:
- **Carga las imÃ¡genes solo cuando las necesita** (lazy loading)
- **Procesa las imÃ¡genes en paralelo** para mejor rendimiento
- **Usa prefetching** para preparar el siguiente batch mientras entrena el actual
- **Optimiza automÃ¡ticamente** el uso de memoria

## ğŸ“‹ CÃ³mo usar la nueva implementaciÃ³n:

### OpciÃ³n 1: Usar el archivo modificado
```python
python train.py  # Tu archivo modificado
```

### OpciÃ³n 2: Usar el archivo optimizado (recomendado)
```python
python train_efficient.py  # VersiÃ³n completamente optimizada
```

### OpciÃ³n 3: Usar manualmente en tu cÃ³digo
```python
from unet.utils.dataset_efficient import create_train_validation_datasets

# Crear datasets eficientes
train_dataset, val_dataset = create_train_validation_datasets(
    images_dir="path/to/images",
    masks_dir="path/to/masks", 
    target_shape_img=[128, 128, 3],
    target_shape_mask=[128, 128, 1],
    batch_size=16,
    validation_split=0.2
)

# Entrenar directamente
model.fit(train_dataset, validation_data=val_dataset, epochs=10)
```

## ğŸ¯ Ventajas de la nueva implementaciÃ³n:

1. **Memoria constante**: Solo mantiene en memoria un batch a la vez
2. **Procesamiento paralelo**: Las imÃ¡genes se procesan usando mÃºltiples cores
3. **Prefetching**: Mientras entrena un batch, prepara el siguiente
4. **Reproducible**: Usa seeds para resultados consistentes
5. **Flexible**: FÃ¡cil de modificar batch size y parÃ¡metros

## âš™ï¸ Optimizaciones adicionales incluidas:

- **Callbacks inteligentes**: ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
- **GestiÃ³n automÃ¡tica de GPU**: Configura memoria dinÃ¡mica si hay GPU disponible
- **Visualizaciones**: Genera grÃ¡ficos automÃ¡ticos del entrenamiento
- **Logging mejorado**: InformaciÃ³n clara del progreso

## ğŸ”§ ParÃ¡metros configurables:

```python
batch_size = 16        # Ajusta segÃºn tu RAM/GPU
validation_split = 0.2 # 20% para validaciÃ³n
target_shape = [128, 128, 3]  # TamaÃ±o de imagen
```

**Resultado**: Tu entrenamiento usarÃ¡ mucha menos RAM y serÃ¡ mÃ¡s eficiente! ğŸ‰
